@misc{agarwal2011fast,
    title={Fast global convergence of gradient methods for high-dimensional statistical recovery},
    author={Alekh Agarwal and Sahand N. Negahban and Martin J. Wainwright},
    year={2011},
    eprint={1104.4824},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{Ras,
 author = {Raskutti, Garvesh and Wainwright, Martin J. and Yu, Bin},
 title = {Restricted Eigenvalue Properties for Correlated Gaussian Designs},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = aug,
 year = {2010},
 issn = {1532-4435},
 pages = {2241--2259},
 numpages = {19},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1859929},
 acmid = {1859929},
 publisher = {JMLR.org},
} 

@article{NedicA2009DSMf,
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
pages = {48--61},
volume = {54},
publisher = {IEEE},
number = {1},
year = {2009},
title = {Distributed Subgradient Methods for Multi-Agent Optimization},
language = {eng},
author = {Nedic, A and Ozdaglar, A},
keywords = {Optimization Methods ; Resource Management ; Cost Function ; Distributed Computing ; Computational Modeling ; Network Topology ; Distributed Control ; Convergence ; Character Generation ; Large-Scale Systems ; Convex Optimization ; Cooperative Control ; Distributed Optimization ; Multi-Agent Network ; Subgradient Method ; Engineering},
}

@misc{ChenAnnieI-An2012Fdfm,
publisher = {Massachusetts Institute of Technology},
year = {2012},
title = {Fast distributed first-order methods},
language = {eng},
author = {Chen, Annie I-An},
keywords = {Electrical Engineering And Computer Science.},
}
@article{Yuan_2016,
   title={On the Convergence of Decentralized Gradient Descent},
   volume={26},
   ISSN={1095-7189},
   url={http://dx.doi.org/10.1137/130943170},
   DOI={10.1137/130943170},
   number={3},
   journal={SIAM Journal on Optimization},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   author={Yuan, Kun and Ling, Qing and Yin, Wotao},
   year={2016},
   month={Jan},
   pages={1835–1854}
}
@article{Bellec_2018,
   title={Slope meets Lasso: Improved oracle bounds and optimality},
   volume={46},
   ISSN={0090-5364},
   url={http://dx.doi.org/10.1214/17-AOS1670},
   DOI={10.1214/17-aos1670},
   number={6B},
   journal={The Annals of Statistics},
   publisher={Institute of Mathematical Statistics},
   author={Bellec, Pierre C. and Lecué, Guillaume and Tsybakov, Alexandre B.},
   year={2018},
   month={Dec},
   pages={3603–3642}
}
@misc{shamir2013communication,
    title={Communication Efficient Distributed Optimization using an Approximate Newton-type Method},
    author={Ohad Shamir and Nathan Srebro and Tong Zhang},
    year={2013},
    eprint={1312.7853},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{bellec2016bounds,
    title={Bounds on the prediction error of penalized least squares estimators with convex penalty},
    author={Pierre C. Bellec and Alexandre B. Tsybakov},
    year={2016},
    eprint={1609.06675},
    archivePrefix={arXiv},
    primaryClass={math.ST}
}
@book{vershynin_2018, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Probability: An Introduction with Applications in Data Science}, DOI={10.1017/9781108231596}, publisher={Cambridge University Press}, author={Vershynin, Roman}, year={2018}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}
@INPROCEEDINGS{6260326,
  author={Y. {Zhang} and H. {Wu} and L. {Cheng}},
  booktitle={2012 Proceedings of International Conference on Modelling, Identification and Control}, 
  title={Some new deformation formulas about variance and covariance}, 
  year={2012},
  volume={},
  number={},
  pages={987-992},
  doi={}}
  @article{Bickel_2009,
   title={Simultaneous analysis of Lasso and Dantzig selector},
   volume={37},
   ISSN={0090-5364},
   url={http://dx.doi.org/10.1214/08-AOS620},
   DOI={10.1214/08-aos620},
   number={4},
   journal={The Annals of Statistics},
   publisher={Institute of Mathematical Statistics},
   author={Bickel, Peter J. and Ritov, Ya’acov and Tsybakov, Alexandre B.},
   year={2009},
   month={Aug},
   pages={1705–1732}
}
@book{10.5555/2834535,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
year = {2015},
isbn = {1498712169},
publisher = {Chapman & Hall/CRC},
abstract = {Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of 1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and group lasso, and review numerical methods for optimization. They also present statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods, and recently developed approaches. In addition, the book examines matrix decomposition, sparse multivariate analysis, graphical models, and compressed sensing. It concludes with a survey of theoretical results for the lasso. In this age of big data, the number of features measured on a person or object can be large and might be larger than the number of observations. This book shows how the sparsity assumption allows us to tackle these problems and extract useful and reproducible patterns from big datasets. Data analysts, computer scientists, and theorists will appreciate this thorough and up-to-date treatment of sparse statistical modeling.}
}

